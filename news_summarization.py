import psycopg2
import pandas as pd
import nltk
import re
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import html

# ‚úÖ Download NLTK dependencies
# nltk.download("stopwords")
# nltk.download("punkt")
from nltk.tokenize import sent_tokenize, word_tokenize

# ‚úÖ Database Connection
DB_URL = "postgresql://admin:xRZQdDx9_04APJdJc1CftnyqX9VZyX@ap-south-1.d67e1e29-cc8d-4b15-8cf9-4ea1e5bd8b9f.aws.yugabyte.cloud:5433/my_database?ssl=true&sslmode=verify-full&sslrootcert=Usfeul info\\root.crt"

def fetch_news_data():
    """Fetch processed news content from the database."""
    conn = psycopg2.connect(DB_URL)
    query = "SELECT news_id,title, raw_content FROM news order by 1;"  # ‚úÖ Using raw_content and cleaning before summarization
    df = pd.read_sql(query, conn)
    conn.close()
    return df

def clean_text(text):
    """Cleans unwanted words, hyperlinks, and HTML while preserving useful content."""

    if not text or pd.isna(text):
        return "Content not available."

    # ‚úÖ Parse HTML properly
    soup = BeautifulSoup(text, "html.parser")

    # ‚úÖ Extract text from <p> tags (ignore <aside>, <blockquote>, <figure>)
    paragraphs = [p.get_text() for p in soup.find_all("p")]
    text = " ".join(paragraphs)

    # ‚úÖ Remove extra spaces, unwanted words, and links
    text = re.sub(r"https?://\S+|www\.\S+", "", text)  # Remove links
    text = re.sub(r"\b(gmt|related|click|read more|subscribe)\b", "", text, flags=re.IGNORECASE)  # Remove noise
    text = re.sub(r"\s+", " ", text).strip()  # Normalize spaces

    if not text.strip():
        return "Content not available."

    return text


def summarize_text(text, news_id, title, word_limit=60):
    """Summarizes text while ensuring a minimum valid output."""
    sentences = sent_tokenize(text)

    # ‚úÖ Print which news article is being processed
    print(f"\nüîπ Processing News ID: {news_id} | Title: {title}")

    if len(sentences) == 0:
        print("‚ùå Debug: No sentences detected after tokenization.")
        return "Summary not available."

    if len(word_tokenize(text)) <= word_limit:
        print("‚úÖ Debug: Text is already within 60 words, returning full cleaned text.")
        return text  

    # ‚úÖ Debug: Check number of sentences
    print(f"üîç Debug: Sentences detected for summarization: {len(sentences)}")

    # ‚úÖ Convert sentences to TF-IDF vectors
    vectorizer = TfidfVectorizer(stop_words="english")
    
    try:
        sentence_vectors = vectorizer.fit_transform(sentences)
    except ValueError:
        print("‚ùå Debug: TF-IDF failed due to very short input.")
        return sentences[0]  

    # ‚úÖ Compute sentence similarity matrix
    similarity_matrix = cosine_similarity(sentence_vectors)
    
    # ‚úÖ Apply PageRank algorithm
    nx_graph = nx.from_numpy_array(similarity_matrix)
    scores = nx.pagerank(nx_graph)

    # ‚úÖ Debug: Check sentence scores
    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)
    print(f"üîç Debug: Ranked sentences count: {len(ranked_sentences)}")

    # ‚úÖ Select sentences within 60 words
    summary = []
    word_count = 0
    
    for _, sentence in ranked_sentences:
        sent_word_count = len(word_tokenize(sentence))
        if word_count + sent_word_count > word_limit:
            break
        summary.append(sentence.strip())
        word_count += sent_word_count

    if not summary:
        print("‚ùå Debug: No sentences added to summary.")
        return sentences[0]  

    print("‚úÖ Debug: Final summary generated successfully!\n")
    return " ".join(summary)

def summarize_news():
    """Main function to fetch news, summarize, and store in DB."""
    df = fetch_news_data()
    df["summary"] = df.apply(lambda row: summarize_text(clean_text(row["raw_content"]), row["news_id"], row["title"], word_limit=60), axis=1)

    conn = psycopg2.connect(DB_URL)
    cursor = conn.cursor()

    for i, row in df.iterrows():
        query = """
        INSERT INTO news_summaries (news_id, summary) VALUES (%s, %s)
        ON CONFLICT (news_id) DO UPDATE SET summary = EXCLUDED.summary;
        """
        cursor.execute(query, (row["news_id"], row["summary"]))

    conn.commit()
    conn.close()
    print("‚úÖ News summaries stored in the database with a strict 60-word limit!")


